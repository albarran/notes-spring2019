---
title: "Text Mining Hands-on"
author: 
  name: Itamar Caspi
  affiliation: Hebrew University, ML for Economists, 2019
  email: caspi.itamar@gmail.com
date: "May 26, 2019 (updated: `r Sys.Date()`)"
output: 
  html_document:
    theme: flatly
    highlight: haddock 
    toc: yes
    toc_depth: 4
    toc_float: yes
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, dpi=300)
```


Okay, let's install (if necessary) the packages we need. We will load each package as we proceed.

```{r packages, message=FALSE}

if (!require("pacman")) install.packages("pacman")

pacman::p_install(quanteda,
                  readtext,
                  tidyverse,
                  readxl,
                  topicmodels,
                  stm,
                  tidytext,
                  glmnet)

theme_set(theme_light())
```

---

# Read data

our data is stored in pdf files. 

```{r}
boi_raw_text <- readtext("11-text-mining/docs/*",
                        docvarsfrom = "filenames",
                        docvarsname = c("year", "month"),
                        dvsep = "-")
  
```
 

```{r}
library(quanteda)

boi_corpus <- corpus(boi_raw_text)
```



```{r }

month_names <- c("January", "February", "March", "April", "May", "June",
           "July", "August", "September", "October", "November",
           "December")

boi_dfm_raw <- boi_corpus %>% 
  dfm(remove = c(stopwords(("english")), month_names),
      remove_punct = TRUE,
      remove_numbers = TRUE,
      stem = TRUE) %>% 
  dfm_trim(min_termfreq = 10,
           min_docfreq = 2)


boi_dfm_raw

topfeatures(boi_dfm_raw)
```


## Multi-word expressions

```{r}

textstat_collocations(boi_corpus)

multiword <- c("interest rate", "Bank of Israel", "committee members",
               "monetary committee", "economic activity", "housing market",
               "global economy")

boi_dfm_multi <- tokens(boi_corpus) %>% 
  tokens_compound(pattern = phrase(multiword)) %>% 
  dfm(remove = c(stopwords(("english")), month_names, "percent"),
      remove_numbers = TRUE,
      remove_punct = TRUE,
      stem = FALSE) %>% 
  dfm_trim(min_termfreq = 10,
           min_docfreq = 2)


```


## Most frequent words

```{r}
topfeatures(boi_dfm_multi, 20)

boi_dfm_tfmitf <- dfm_tfidf(boi_dfm_multi)

topfeatures(boi_dfm_tfmitf, 20)
```


## Which words predict rate changes?


```{r}

df_chng <- read_excel(path = "11-text-mining/data/rate-changes.xlsx",
                      sheet = "data")

Y <- df_chng %>% 
  select(change) %>% 
  pull()



```


```{r }

library(glmnet)

model <- cv.glmnet(x = boi_dfm_multi, y = Y,
  family = "binomial",
  keep = TRUE
)

```


```{r }

library(broom)

coefs <- model$glmnet.fit %>%
  tidy() %>%
  filter(step == 10)
  #filter(lambda == model$lambda.1se)

```

```{r }
library(drlib)

coefs %>%
  group_by(estimate > 0) %>%
  top_n(10, abs(estimate)) %>%
  ungroup() %>%
  ggplot(aes(fct_reorder(term, estimate), estimate, fill = estimate > 0)) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  coord_flip() +
  labs(
    x = NULL,
    title = "Coefficients that increase/decrease probability of rate change"
  )

```


## Topic modeling

```{r lda_boi, eval = FALSE}

library(stm)
library(topicmodels)

topic_model <- LDA(convert(boi_dfm_multi, to = "topicmodels"), k = 10)
get_terms(lda, 5)

topic_model <- stm(boi_dfm_multi, K = 4, verbose = FALSE)
```


```{r}
library(drlib)

td_beta <- tidy(topic_model)

td_beta %>%
    group_by(topic) %>%
    top_n(10, beta) %>%
    ungroup() %>%
    mutate(topic = paste0("Topic ", topic),
           term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(term, beta, fill = as.factor(topic))) +
    geom_col(alpha = 0.8, show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free_y") +
    coord_flip() +
    scale_x_reordered() +
    labs(x = NULL, y = expression(beta),
         title = "Highest word probabilities for each topic")

```


```{r}
td_gamma <- tidy(topic_model, matrix = "gamma",                    
                 document_names = rownames(dfm))

ggplot(td_gamma, aes(gamma, fill = as.factor(topic))) +
  geom_histogram(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~ topic, ncol = 2) +
  labs(title = "Distribution of document probabilities for each topic",
       y = "Number of minutes",
       x = expression(gamma))
```


```{r optimal_k, eval=FALSE}
source("11-text-mining/optimal_k.R")

dtm <- convert(boi_dfm_multi, to = "topicmodels")

control <- list(burnin = 500, iter = 1000, keep = 100)

(k <- optimal_k(dtm, 15, control = control))
```
